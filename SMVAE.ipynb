{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import BRICS\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import NNConv, GlobalAttention, GCNConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.distributions import Bernoulli, Categorical\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid molecules: 12500\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_df = pd.read_csv('BRD4_mini_sampled_data_POS.csv')  # Replace with your dataset path\n",
    "\n",
    "# Filter out invalid SMILES\n",
    "valid_smiles = []\n",
    "for idx, row in data_df.iterrows():\n",
    "    smiles = row['molecule_smiles']\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        valid_smiles.append(smiles)\n",
    "\n",
    "print(f\"Total valid molecules: {len(valid_smiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_features(atom):\n",
    "    # Atomic number\n",
    "    atomic_number = atom.GetAtomicNum()\n",
    "    # Atom degree\n",
    "    degree = atom.GetDegree()\n",
    "    # Formal charge\n",
    "    formal_charge = atom.GetFormalCharge()\n",
    "    # Number of hydrogens\n",
    "    num_hs = atom.GetTotalNumHs()\n",
    "    # Aromaticity\n",
    "    is_aromatic = atom.GetIsAromatic()\n",
    "    # Hybridization\n",
    "    hybridization = atom.GetHybridization()\n",
    "\n",
    "    # One-hot encoding for hybridization\n",
    "    hybridization_encoding = [0]*6\n",
    "    hybridization_types = [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2,\n",
    "        Chem.rdchem.HybridizationType.UNSPECIFIED\n",
    "    ]\n",
    "    if hybridization in hybridization_types:\n",
    "        idx = hybridization_types.index(hybridization)\n",
    "        hybridization_encoding[idx] = 1\n",
    "\n",
    "    # Combine all features into a list\n",
    "    features = [\n",
    "        atomic_number,\n",
    "        degree,\n",
    "        formal_charge,\n",
    "        num_hs,\n",
    "        int(is_aromatic)\n",
    "    ] + hybridization_encoding\n",
    "    return features\n",
    "\n",
    "def bond_features(bond):\n",
    "    bond_type = bond.GetBondType()\n",
    "\n",
    "    # Bond type as one-hot encoding\n",
    "    bond_type_feats = [\n",
    "        int(bond_type == Chem.rdchem.BondType.SINGLE),\n",
    "        int(bond_type == Chem.rdchem.BondType.DOUBLE),\n",
    "        int(bond_type == Chem.rdchem.BondType.TRIPLE),\n",
    "        int(bond_type == Chem.rdchem.BondType.AROMATIC)\n",
    "    ]\n",
    "\n",
    "    # Conjugation\n",
    "    conjugation_feat = [int(bond.GetIsConjugated())]\n",
    "\n",
    "    # Ring membership\n",
    "    ring_feat = [int(bond.IsInRing())]\n",
    "\n",
    "    # Stereo configuration as one-hot encoding\n",
    "    stereo = bond.GetStereo()\n",
    "    stereo_feats = [\n",
    "        int(stereo == Chem.rdchem.BondStereo.STEREONONE),\n",
    "        int(stereo == Chem.rdchem.BondStereo.STEREOANY),\n",
    "        int(stereo == Chem.rdchem.BondStereo.STEREOZ),\n",
    "        int(stereo == Chem.rdchem.BondStereo.STEREOE),\n",
    "        int(stereo == Chem.rdchem.BondStereo.STEREOCIS),\n",
    "        int(stereo == Chem.rdchem.BondStereo.STEREOTRANS)\n",
    "    ]\n",
    "\n",
    "    # Combine all features\n",
    "    bond_feats = bond_type_feats + conjugation_feat + ring_feat + stereo_feats\n",
    "    return bond_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_features(frag):\n",
    "    # Compute atom features\n",
    "    atom_feats = []\n",
    "    for atom in frag.GetAtoms():\n",
    "        feat = atom_features(atom)\n",
    "        atom_feats.append(feat)\n",
    "    # Aggregate atom features (sum)\n",
    "    atom_feats = np.array(atom_feats)\n",
    "    atom_feat_vector = np.sum(atom_feats, axis=0)\n",
    "\n",
    "    # Compute bond features\n",
    "    bond_feats = []\n",
    "    bonds = frag.GetBonds()\n",
    "    if bonds:\n",
    "        for bond in bonds:\n",
    "            feat = bond_features(bond)\n",
    "            bond_feats.append(feat)\n",
    "        bond_feats = np.array(bond_feats)\n",
    "        bond_feat_vector = np.sum(bond_feats, axis=0)\n",
    "    else:\n",
    "        # Handle fragments with no bonds (e.g., single atoms)\n",
    "        bond_feat_vector = np.zeros(13)\n",
    "\n",
    "    # Concatenate atom and bond feature vectors\n",
    "    frag_feat_vector = np.concatenate([atom_feat_vector, bond_feat_vector])\n",
    "    return frag_feat_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_molecule(mol):\n",
    "    # Find BRICS bonds to break\n",
    "    brics_bonds = BRICS.FindBRICSBonds(mol)\n",
    "    bonds_to_break = []\n",
    "    for (atom_idx1, atom_idx2), (label1, label2) in brics_bonds:\n",
    "        bond = mol.GetBondBetweenAtoms(atom_idx1, atom_idx2)\n",
    "        if bond is not None:\n",
    "            bonds_to_break.append(bond.GetIdx())\n",
    "\n",
    "    # Break the bonds\n",
    "    fragment_mol = Chem.FragmentOnBonds(mol, bonds_to_break, addDummies=True)\n",
    "\n",
    "    # Get the fragments\n",
    "    fragments = Chem.GetMolFrags(fragment_mol, asMols=True, sanitizeFrags=True)\n",
    "\n",
    "    # Build mapping from fragment to fragment ID\n",
    "    frag_id_mapping = {}\n",
    "    for idx, frag in enumerate(fragments):\n",
    "        frag_id_mapping[idx] = frag\n",
    "\n",
    "    # Build the tree by connecting fragments via attachment points\n",
    "    # Attachment points are marked with dummy atoms (*)\n",
    "    # We can find which fragments are connected by looking at the dummy atoms\n",
    "    # Each dummy atom has an isotope number indicating the original bond\n",
    "\n",
    "    # Build a mapping from dummy atom isotope labels to fragment IDs and atom indices\n",
    "    dummy_atom_mapping = {}\n",
    "    for idx, frag in frag_id_mapping.items():\n",
    "        for atom in frag.GetAtoms():\n",
    "            if atom.GetAtomicNum() == 0:  # Dummy atom (*)\n",
    "                isotope = atom.GetIsotope()\n",
    "                if isotope not in dummy_atom_mapping:\n",
    "                    dummy_atom_mapping[isotope] = []\n",
    "                dummy_atom_mapping[isotope].append((idx, atom.GetIdx(), atom.GetAtomMapNum()))\n",
    "\n",
    "    # Build the tree edges with bond features\n",
    "    tree_edges = []\n",
    "    edge_features = []\n",
    "    for isotope, connections in dummy_atom_mapping.items():\n",
    "        if len(connections) == 2:\n",
    "            frag1_id, atom1_idx, atom1_mapnum = connections[0]\n",
    "            frag2_id, atom2_idx, atom2_mapnum = connections[1]\n",
    "\n",
    "            # Since the original bond was broken, we might not have bond features\n",
    "            bond_feat = np.zeros(13)  # Adjust the size based on your bond feature length\n",
    "\n",
    "            tree_edges.append((frag1_id, frag2_id))\n",
    "            edge_features.append(bond_feat)\n",
    "        else:\n",
    "            # Handle cases with more than two connections (e.g., branching)\n",
    "            for i in range(len(connections)):\n",
    "                for j in range(i+1, len(connections)):\n",
    "                    frag1_id, atom1_idx, atom1_mapnum = connections[i]\n",
    "                    frag2_id, atom2_idx, atom2_mapnum = connections[j]\n",
    "\n",
    "                    bond_feat = np.zeros(13)  # Adjust the size based on your bond feature length\n",
    "\n",
    "                    tree_edges.append((frag1_id, frag2_id))\n",
    "                    edge_features.append(bond_feat)\n",
    "\n",
    "    return frag_id_mapping, tree_edges, edge_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "substructure_library = {}\n",
    "substructure_vocab = []\n",
    "substructure_to_idx = {}\n",
    "idx_to_substructure = {}\n",
    "label_counter = 0\n",
    "\n",
    "def mol_to_substructure_tree(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None  # Skip invalid molecules\n",
    "\n",
    "    # Decompose molecule into fragments using BRICS\n",
    "    frag_id_mapping, tree_edges, edge_features = decompose_molecule(mol)\n",
    "\n",
    "    # For each fragment, compute features and assign labels\n",
    "    fragment_features_list = []\n",
    "    fragment_labels = []\n",
    "    for frag_id in sorted(frag_id_mapping.keys()):\n",
    "        frag = frag_id_mapping[frag_id]\n",
    "        feat_vector = fragment_features(frag)\n",
    "        # Use the canonical SMILES of the fragment\n",
    "        smiles_frag = Chem.MolToSmiles(frag, isomericSmiles=True)\n",
    "        # Add to substructure library\n",
    "        if smiles_frag not in substructure_to_idx:\n",
    "            substructure_to_idx[smiles_frag] = len(substructure_vocab)\n",
    "            idx_to_substructure[len(substructure_vocab)] = smiles_frag\n",
    "            substructure_vocab.append(smiles_frag)\n",
    "        frag_label = substructure_to_idx[smiles_frag]\n",
    "        fragment_features_list.append(feat_vector)\n",
    "        fragment_labels.append(frag_label)\n",
    "\n",
    "    # Prepare edge indices and edge attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for idx, (i, j) in enumerate(tree_edges):\n",
    "        edge_index.append([i, j])\n",
    "        edge_index.append([j, i])  # Undirected graph\n",
    "        bond_feat = edge_features[idx]\n",
    "        edge_attr.append(bond_feat)\n",
    "        edge_attr.append(bond_feat)  # Duplicate for both directions\n",
    "\n",
    "    # Convert to tensors\n",
    "    x = torch.tensor(fragment_features_list, dtype=torch.float)\n",
    "    x = x / x.norm(dim=1, keepdim=True)  # Normalize node features\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    y = torch.tensor(fragment_labels, dtype=torch.long)  # Substructure labels\n",
    "\n",
    "    # Create Data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12500 [00:00<?, ?it/s]/var/folders/_y/zf2b46fx13x7dcn3m4xk2bh00000gn/T/ipykernel_20560/442557517.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  x = torch.tensor(fragment_features_list, dtype=torch.float)\n",
      "100%|██████████| 12500/12500 [04:46<00:00, 43.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data samples: 12500\n",
      "Substructure vocabulary size: 10106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_list = []\n",
    "for smiles in tqdm(valid_smiles, total=len(valid_smiles)):\n",
    "    data = mol_to_substructure_tree(smiles)\n",
    "    if data is not None:\n",
    "        data_list.append(data)\n",
    "\n",
    "print(f\"Total data samples: {len(data_list)}\")\n",
    "print(f\"Substructure vocabulary size: {len(substructure_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_substructure_library(substructure_vocab, filename='substructureLibrary.csv'):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Index', 'SMILES'])\n",
    "        for idx, smiles in enumerate(substructure_vocab):\n",
    "            writer.writerow([idx, smiles])\n",
    "\n",
    "write_substructure_library(substructure_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNEncoder(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, latent_dim):\n",
    "        super(MPNNEncoder, self).__init__()\n",
    "\n",
    "        # Define the network to compute edge weights\n",
    "        self.nn_edge1 = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, node_feature_dim * hidden_dim)\n",
    "        )\n",
    "        self.nn_edge2 = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Message Passing Layers\n",
    "        self.conv1 = NNConv(node_feature_dim, hidden_dim, self.nn_edge1, aggr='mean')\n",
    "        self.conv2 = NNConv(hidden_dim, hidden_dim, self.nn_edge2, aggr='mean')\n",
    "\n",
    "        # Attention-based pooling using softmax\n",
    "        self.attention = GlobalAttention(\n",
    "            gate_nn=nn.Sequential(nn.Linear(hidden_dim, 1)),\n",
    "            nn=nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        )\n",
    "\n",
    "        # Latent space projections\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device)\n",
    "\n",
    "        # Message Passing\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Attention-based Pooling\n",
    "        x = self.attention(x, data.batch)\n",
    "\n",
    "        # Latent Space\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, substructure_vocab_size):\n",
    "        super(TreeDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.substructure_vocab_size = substructure_vocab_size\n",
    "\n",
    "        # Initial hidden state projection from latent vector\n",
    "        self.fc_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_root = nn.Linear(latent_dim, substructure_vocab_size)\n",
    "\n",
    "        # RNN cell for tree traversal\n",
    "        self.rnn_cell = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Child existence predictor\n",
    "        self.fc_exist = nn.Linear(hidden_dim, 1)  # Outputs logit for sigmoid\n",
    "\n",
    "        # Substructure selector\n",
    "        self.fc_substruct = nn.Linear(hidden_dim, substructure_vocab_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        hidden = self.fc_hidden(z)  # Initial hidden state\n",
    "        root_logits = self.fc_root(z)  # Root substructure logits\n",
    "\n",
    "        # Use Gumbel-Softmax to sample root substructure\n",
    "        root_prob = F.gumbel_softmax(root_logits, tau=1, hard=True)\n",
    "        root_substruct = root_prob.argmax(dim=1)\n",
    "\n",
    "        # Initialize outputs\n",
    "        trees = []  # List of generated trees for each sample\n",
    "        decisions_list = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            tree = []\n",
    "            decisions = []\n",
    "\n",
    "            # Initialize stack for DFS\n",
    "            stack = []\n",
    "            node = {\n",
    "                'hidden': hidden[i],\n",
    "                'substruct': root_substruct[i],\n",
    "                'parent': None,\n",
    "                'children': []\n",
    "            }\n",
    "            tree.append(node)\n",
    "            stack.append(node)\n",
    "\n",
    "            # Traverse the tree\n",
    "            while stack:\n",
    "                current_node = stack.pop()\n",
    "                h = current_node['hidden']\n",
    "\n",
    "                # Decide whether to create a child\n",
    "                exist_logit = self.fc_exist(h)\n",
    "                exist_prob = torch.sigmoid(exist_logit)\n",
    "                # During training, use the probability; during inference, sample\n",
    "                exist_decision = (exist_prob > 0.5).float()\n",
    "                decisions.append({'exist_prob': exist_prob, 'exist_decision': exist_decision})\n",
    "\n",
    "                if exist_decision.item() == 1:\n",
    "                    # Decide which substructure to use\n",
    "                    substruct_logits = self.fc_substruct(h)\n",
    "                    substruct_prob = F.gumbel_softmax(substruct_logits, tau=1, hard=True)\n",
    "                    substruct_idx = substruct_prob.argmax(dim=0)\n",
    "                    decisions.append({'substruct_prob': substruct_prob, 'substruct_idx': substruct_idx})\n",
    "\n",
    "                    # Create child node\n",
    "                    h_child = self.rnn_cell(h, h)  # Update hidden state\n",
    "                    child_node = {\n",
    "                        'hidden': h_child,\n",
    "                        'substruct': substruct_idx,\n",
    "                        'parent': current_node,\n",
    "                        'children': []\n",
    "                    }\n",
    "                    current_node['children'].append(child_node)\n",
    "                    stack.append(child_node)\n",
    "\n",
    "            trees.append(tree)\n",
    "            decisions_list.append(decisions)\n",
    "\n",
    "        return trees, decisions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, latent_dim, substructure_vocab_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = MPNNEncoder(node_feature_dim, edge_feature_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = TreeDecoder(latent_dim, hidden_dim, substructure_vocab_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std).to(device)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, data):\n",
    "        mu, logvar = self.encoder(data)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        trees, decisions = self.decoder(z)\n",
    "        return trees, decisions, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def compute_loss(decisions_list, data, substructure_vocab_size):\n",
    "    total_exist_loss = 0\n",
    "    total_substruct_loss = 0\n",
    "\n",
    "    # Since we have variable-length outputs, we need to handle each sample individually\n",
    "    for decisions, sample_data in zip(decisions_list, data):\n",
    "        # Ground truth tree from sample_data\n",
    "        ground_truth_labels = sample_data.y  # Substructure labels\n",
    "        # For simplicity, assume a linearized version of the tree (you may need to adjust this)\n",
    "        gt_substruct_indices = ground_truth_labels.tolist()\n",
    "\n",
    "        # Initialize pointers\n",
    "        gt_idx = 0\n",
    "        for decision in decisions:\n",
    "            if 'exist_prob' in decision:\n",
    "                # Binary cross-entropy loss for existence decision\n",
    "                target = torch.tensor([1.0]).to(device) if gt_idx < len(gt_substruct_indices) - 1 else torch.tensor([0.0]).to(device)\n",
    "                exist_loss = F.binary_cross_entropy(decision['exist_prob'], target)\n",
    "                total_exist_loss += exist_loss\n",
    "\n",
    "            if 'substruct_prob' in decision:\n",
    "                if gt_idx < len(gt_substruct_indices):\n",
    "                    target = torch.tensor([gt_substruct_indices[gt_idx]]).to(device)\n",
    "                    substruct_loss = F.cross_entropy(decision['substruct_prob'].unsqueeze(0), target)\n",
    "                    total_substruct_loss += substruct_loss\n",
    "                    gt_idx += 1\n",
    "\n",
    "    total_loss = total_exist_loss + total_substruct_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/an/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/Users/an/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Prepare DataLoader\n",
    "batch_size = 16\n",
    "loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "node_feature_dim = data_list[0].x.shape[1]\n",
    "edge_feature_dim = data_list[0].edge_attr.shape[1]\n",
    "hidden_dim = 64\n",
    "latent_dim = 32\n",
    "substructure_vocab_size = len(substructure_vocab)\n",
    "\n",
    "model = VAE(node_feature_dim, edge_feature_dim, hidden_dim, latent_dim, substructure_vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        trees, decisions_list, mu, logvar = model(data)\n",
    "        loss_recon = compute_loss(decisions_list, data, substructure_vocab_size)\n",
    "        loss_kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        loss = loss_recon + loss_kl\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_molecule_from_tree(tree, idx_to_substructure):\n",
    "    try:\n",
    "        # Initialize molecule\n",
    "        mol = None\n",
    "        atom_idx_map = {}\n",
    "        for node in tree:\n",
    "            substruct_idx = node['substruct'].item()\n",
    "            smiles_frag = idx_to_substructure[substruct_idx]\n",
    "            frag_mol = Chem.MolFromSmiles(smiles_frag)\n",
    "            if mol is None:\n",
    "                mol = frag_mol\n",
    "                # Map atom indices\n",
    "                atom_idx_map[node['substruct']] = list(range(mol.GetNumAtoms()))\n",
    "            else:\n",
    "                # Attach frag_mol to mol\n",
    "                # This is a simplified example; proper attachment requires handling dummy atoms\n",
    "                mol = Chem.CombineMols(mol, frag_mol)\n",
    "                # Update atom index mapping\n",
    "                atom_idx_map[node['substruct']] = list(range(mol.GetNumAtoms() - frag_mol.GetNumAtoms(), mol.GetNumAtoms()))\n",
    "        # Sanitize molecule\n",
    "        Chem.SanitizeMol(mol)\n",
    "        smiles = Chem.MolToSmiles(mol)\n",
    "        return smiles\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generated_molecules = []\n",
    "num_samples = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_samples):\n",
    "        # Sample z from standard normal distribution\n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        # Condition on 'binds' label = 1 (binding)\n",
    "        condition = torch.tensor([[1.0]]).to(device)\n",
    "        trees, _ = model.decoder(z, condition)\n",
    "\n",
    "        # Reconstruct molecule from generated tree\n",
    "        generated_smiles = reconstruct_molecule_from_tree(trees[0], idx_to_substructure)\n",
    "        if generated_smiles is not None:\n",
    "            generated_molecules.append(generated_smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = 0\n",
    "for smiles in generated_molecules:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        valid_count += 1\n",
    "\n",
    "validity = valid_count / len(generated_molecules)\n",
    "print(f\"Validity of generated molecules: {validity * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
